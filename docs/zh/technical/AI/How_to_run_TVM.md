# TVMå¿«é€Ÿä½¿ç”¨æŒ‡å—

## 1. ç›¸å…³æ ·ä¾‹
**ç¬”è€…åœ¨æœ¬æ–‡ä¸­ä»¥sigmoidç®—å­ä¸ºä¾‹ï¼Œä¸è¿‡ä¼˜åŒ–æ•ˆæœå¹¶ä¸æ˜æ˜¾ï¼Œåç»­æœ‰å¾…å¯»æ‰¾é—®é¢˜æ‰€åœ¨**
```
import tvm
from tvm import te
from tvm import meta_schedule as ms
from tvm.meta_schedule.database import JSONDatabase
import multiprocessing
import numpy as np
import traceback
import os

# ================================================================
# 1. å®šä¹‰è®¡ç®—
# ================================================================
def sigmoid(M, N, dtype="float32"):
    """å®šä¹‰ Sigmoid è®¡ç®—"""
    A = te.placeholder((M, N), name="A", dtype=dtype)
    C = te.compute(
        (M, N),
        lambda i, j: 1 / (1 + tvm.tir.exp(-A[i, j])),
        name="C"
    )
    return te.create_prim_func([A, C])


# ================================================================
# 2. è°ƒä¼˜å¹¶æ„å»ºæ¨¡å—
# ================================================================
def tune_and_build_sigmoid(M, N, dtype="float32", target=None, work_dir="./tune_sigmoid"):
    if target is None:
        num_cores = multiprocessing.cpu_count()
        target = f"llvm -num-cores={num_cores}"

    func = sigmoid(M, N, dtype)

    try:
        print("ğŸš€ å¼€å§‹è‡ªåŠ¨è°ƒä¼˜...")
        result = ms.tune_tir(
            mod=func,
            target=target,
            work_dir=work_dir,
            max_trials_global=100,
            num_trials_per_iter=10,
        )

        print("è°ƒä¼˜è¿”å›ç±»å‹:", type(result))
        print("repr:", repr(result)[:200])

        # ================================================================
        # âœ… å¦‚æœè¿”å› JSONDatabaseï¼Œä»æ•°æ®åº“ä¸­æå–æœ€ä½³ schedule
        # ================================================================
        if isinstance(result, JSONDatabase):
            print("ğŸ“‚ æ£€æµ‹åˆ° JSONDatabaseï¼Œå°è¯•ä»ä¸­åŠ è½½æœ€ä½³è°ƒåº¦...")

            # ä»æ•°æ®åº“ä¸­è·å–æ‰€æœ‰è°ƒä¼˜è®°å½•
            records = list(result.get_all_tuning_records())
            if not records:
                raise RuntimeError("æ•°æ®åº“ä¸ºç©ºï¼Œæœªæ‰¾åˆ°ä»»ä½•è°ƒä¼˜è®°å½•")

            # å–ç¬¬ä¸€ä¸ªè°ƒä¼˜è®°å½•çš„ workloadï¼ˆè¿™é‡Œåªæœ‰ä¸€ä¸ªç®—å­ï¼‰
            workload = records[0].workload

            # è·å–è¯¥ workload çš„æœ€ä¼˜è°ƒä¼˜è®°å½•
            try:
                best_tuning_record = result.get_top_k(workload, top_k=1)[0]
            except TypeError:
                # å¦‚æœæ¥å£å˜åŠ¨ï¼Œå°è¯•æ—§å‚æ•°å
                best_tuning_record = result.get_top_k(workload, k=1)[0]
            print("âœ… å·²æ‰¾åˆ°æœ€ä¼˜è°ƒåº¦è®°å½•ï¼Œå‡†å¤‡æå–è°ƒåº¦...")

            # TVM 0.23.dev0 çš„æ–¹å¼ï¼šé€šè¿‡ as_measure_candidate() å–å‡º schedule
            candidate = best_tuning_record.as_measure_candidate()
            sch = candidate.sch

            print("âœ… è·å–è°ƒåº¦æˆåŠŸï¼Œå¼€å§‹æ„å»ºæ¨¡å—...")
            mod = tvm.build(sch.mod, target=target)
            print("âœ… ä½¿ç”¨æœ€ä¼˜è°ƒåº¦æ„å»ºæˆåŠŸï¼")
            return result, mod, None

        # ================================================================
        # âœ… å¦‚æœè¿”å›çš„æ˜¯ IRModuleï¼ˆæ—§ç‰ˆæœ¬å…¼å®¹ï¼‰
        # ================================================================
        elif isinstance(result, tvm.IRModule):
            print("âœ… æ£€æµ‹åˆ° IRModuleï¼Œç›´æ¥æ„å»ºä¸­...")
            mod = tvm.build(result, target=target)
            return result, mod, None

        else:
            raise RuntimeError(f"æœªçŸ¥çš„è°ƒä¼˜è¿”å›ç±»å‹: {type(result)}")

    except Exception as e:
        print("\nâš ï¸ è‡ªåŠ¨è°ƒä¼˜å¤±è´¥ï¼Œè¿›å…¥å›é€€æ¨¡å¼")
        traceback.print_exc()
        print("è°ƒä¼˜ç›®å½•:", os.path.abspath(work_dir))
        mod = tvm.build(func, target=target)
        return None, mod, e


# ================================================================
# 3. æµ‹è¯•å‡½æ•°
# ================================================================
def test_sigmoid_module(mod, M, N, target="llvm"):
    print("ğŸ§ª æµ‹è¯•æ¨¡å—åŠŸèƒ½ä¸æ€§èƒ½...")

    a_np = np.random.uniform(-5, 5, size=(M, N)).astype("float32")
    c_np = np.zeros_like(a_np)

    dev = tvm.device(target.split()[0], 0)
    a_tvm = tvm.runtime.tensor(a_np, device=dev)
    c_tvm = tvm.runtime.tensor(c_np, device=dev)
    
    # 2. é¢„çƒ­è¿è¡Œï¼ˆæ’é™¤é¦–æ¬¡è°ƒç”¨ overheadï¼‰
    mod(a_tvm, c_tvm)
    dev.sync()  # ç¡®ä¿é¢„çƒ­å®Œæˆ
    
    # 3. å¤šæ¬¡è¿è¡Œå–å¹³å‡ï¼ˆ100æ¬¡ï¼‰
    import time
    run_times = 100
    start = time.time()
    for _ in range(run_times):
        mod(a_tvm, c_tvm)
    dev.sync()  # ç¡®ä¿æ‰€æœ‰è®¡ç®—å®Œæˆ
    end = time.time()
    
    # 4. è®¡ç®—ç²¾å‡†æ€§èƒ½
    total_time_us = (end - start) * 1e6
    avg_latency_us = total_time_us / run_times
    flop = 3 * M * N  # Sigmoid å•å…ƒç´ è®¡ç®—é‡
    gflops = (flop * run_times) / (total_time_us * 1e3)  # è½¬æ¢ä¸º GFLOPS
    
    # ç²¾åº¦éªŒè¯
    expected = 1 / (1 + np.exp(-a_np))
    diff = np.max(np.abs(c_tvm.numpy() - expected))
    
    # æ‰“å°ç²¾å‡†æ•°æ®
    print(f"ğŸ“Š ç²¾å‡†æ€§èƒ½æŒ‡æ ‡ï¼šGFLOPS = {gflops:.4f} | å¹³å‡å»¶è¿Ÿ = {avg_latency_us:.4f} Î¼s")
    print(f"ğŸ” æœ€å¤§è¯¯å·®ï¼š{diff:.6e}")
    return diff < 1e-5, gflops, avg_latency_us


# ================================================================
# 4. ä¸»å…¥å£
# ================================================================
if __name__ == "__main__":
    M, N = 1024, 1024
    num_cores = multiprocessing.cpu_count()
    target = f"llvm -num-cores={num_cores}"

    print(f"ä½¿ç”¨ç›®æ ‡: {target}")
    print(f"çŸ©é˜µå¤§å°: {M}x{N}")

    # === è°ƒä¼˜ + æ„å»º ===
    result, mod, error = tune_and_build_sigmoid(M, N, target=target)

    if result is not None:
        print("\nğŸ‰ è‡ªåŠ¨è°ƒä¼˜æˆåŠŸï¼")
        if hasattr(result, "trace"):
            print("ä¼˜åŒ–è°ƒåº¦è½¨è¿¹:")
            print(result.trace)
    else:
        print("\nâ„¹ï¸ ä½¿ç”¨é»˜è®¤è°ƒåº¦")
        if error:
            print(f"é”™è¯¯ä¿¡æ¯: {error}")

    # === æµ‹è¯•æ¨¡å— ===
    success = test_sigmoid_module(mod, M, N, target)

    if success:
        print("\nğŸŠ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Sigmoid ä¼˜åŒ–å®Œæˆã€‚")
    else:
        print("\nğŸ’¡ åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è®¡ç®—å®šä¹‰æˆ–è°ƒåº¦ã€‚")

```

**è¾“å‡ºç»“æœåº”ä¸º**
```
è°ƒä¼˜è¿”å›ç±»å‹: <class 'tvm.meta_schedule.database.json_database.JSONDatabase'>
repr: meta_schedule.JSONDatabase(0x600001070ea8)
ğŸ“‚ æ£€æµ‹åˆ° JSONDatabaseï¼Œå°è¯•ä»ä¸­åŠ è½½æœ€ä½³è°ƒåº¦...
âœ… å·²æ‰¾åˆ°æœ€ä¼˜è°ƒåº¦è®°å½•ï¼Œå‡†å¤‡æå–è°ƒåº¦...
âœ… è·å–è°ƒåº¦æˆåŠŸï¼Œå¼€å§‹æ„å»ºæ¨¡å—...
âœ… ä½¿ç”¨æœ€ä¼˜è°ƒåº¦æ„å»ºæˆåŠŸï¼

ğŸ‰ è‡ªåŠ¨è°ƒä¼˜æˆåŠŸï¼
ğŸ§ª æµ‹è¯•æ¨¡å—åŠŸèƒ½ä¸æ€§èƒ½...
ğŸ“Š ç²¾å‡†æ€§èƒ½æŒ‡æ ‡ï¼šGFLOPS = 12.0841 | å¹³å‡å»¶è¿Ÿ = 260.3197 Î¼s
ğŸ” æœ€å¤§è¯¯å·®ï¼š0.000000e+00

ğŸŠ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Sigmoid ä¼˜åŒ–å®Œæˆã€‚
```

å…¶ä¸­æ ¸å¿ƒAPIä¸ºï¼š
- è®¡ç®—å®šä¹‰ï¼šte.placeholder, te.compute, tvm.tir.exp, te.create_prim_func
- è‡ªåŠ¨è°ƒä¼˜ï¼ˆMetaScheduleï¼‰ï¼šms.tune_tir, JSONDatabase, as_measure_candidate, tvm.build
- æµ‹è¯•å’Œæ€§èƒ½æµ‹é‡ï¼štvm.runtime.tensor, mod(), dev.sync(), numpy å¯¹æ¯”éªŒè¯

## 2.æ ¸å¿ƒAPIä»‹ç»

### 1.è®¡ç®—å®šä¹‰ç›¸å…³ (tvm.te, tvm.tir)
è¿™äº› API ç”¨äºå®šä¹‰è®¡ç®—å›¾ï¼ˆå³ç®—å­é€»è¾‘ï¼‰ï¼Œå±äº TVM çš„ Tensor Expression (TE) å±‚ã€‚

1. [tvm.te.placeholder(shape, dtype=None, name='placeholder')](https://tvm.apache.org/docs/reference/api/python/te.html#tvm.te.placeholder) 

    ä½œç”¨:
    åˆ›å»ºä¸€ä¸ªè¾“å…¥å¼ é‡ï¼ˆå ä½ç¬¦ï¼‰ï¼Œç›¸å½“äºå®šä¹‰è®¡ç®—çš„è¾“å…¥å˜é‡ã€‚

    `A = te.placeholder((M, N), name="A", dtype="float32")`

    å‚æ•°è¯´æ˜

    | å‚æ•°      | å«ä¹‰                        |
    | ------- | ------------------------- |
    | `shape` | å¼ é‡çš„å½¢çŠ¶ (tuple æˆ– list)      |
    | `name`  | å¼ é‡åç§°ï¼ˆä¾¿äºè°ƒè¯•ï¼‰                |
    | `dtype` | æ•°æ®ç±»å‹ï¼Œå¦‚ "float32", "int32" |

    **è¿”å›å€¼**

    - ä¸€ä¸ª tvm.te.Tensor å¯¹è±¡ï¼Œå¯ä»¥åœ¨ te.compute ä¸­ä½¿ç”¨ã€‚

    **âš ï¸æ³¨æ„ç‚¹**
    - placeholder ä»…å®šä¹‰è¾“å…¥ï¼Œä¸å«å…·ä½“æ•°å€¼ï¼›
    - shape å¿…é¡»æ˜¯é™æ€ï¼ˆå¸¸æ•°ï¼‰å½¢çŠ¶ï¼›
    - dtype å¿…é¡»æ˜¯ TVM æ”¯æŒçš„ç±»å‹ï¼ˆfloat32 / float16 / int32 / etcï¼‰ã€‚

2. [tvm.te.compute(shape, fcompute, name='compute', tag='', attrs=None, varargs_names=None)](https://tvm.apache.org/docs/reference/api/python/te.html#tvm.te.compute)

    **ä½œç”¨**

    å®šä¹‰è¾“å‡ºå¼ é‡çš„è®¡ç®—é€»è¾‘ã€‚\
    TVM ä¼šåŸºäºè¿™ä¸ªå®šä¹‰æ„å»ºè®¡ç®—å›¾ã€‚

    ```python
    C = te.compute(
        (M, N),
        lambda i, j: 1 / (1 + tvm.tir.exp(-A[i, j])),
        name="C"
    )
    ```
    **å‚æ•°è¯´æ˜**
    | å‚æ•°         | å«ä¹‰                       |
    | ---------- | ------------------------ |
    | `shape`    | è¾“å‡ºå¼ é‡çš„å½¢çŠ¶                  |
    | `fcompute` | ä¸€ä¸ª lambda å‡½æ•°ï¼Œå®šä¹‰æ¯ä¸ªå…ƒç´ çš„è®¡ç®—è§„åˆ™ |
    | `name`     | å¼ é‡åç§°                     |

    è¿”å›å€¼

    - ä¸€ä¸ªæ–°çš„ tvm.te.Tensorï¼Œæè¿°è®¡ç®—ç»“æœã€‚

    æ³¨æ„ç‚¹

    - fcompute çš„å‚æ•°æ•°é‡å¿…é¡»ä¸ shape ç»´åº¦ä¸€è‡´ï¼›

    - ä¸èƒ½åŒ…å« Python æ§åˆ¶æµï¼ˆå¦‚ ifï¼‰ï¼Œå¿…é¡»æ˜¯çº¯ç¬¦å·è®¡ç®—ï¼›

    - è®¡ç®—è¡¨è¾¾å¼ä¸­åº”ä½¿ç”¨ tvm.tir ä¸‹çš„æ•°å­¦å‡½æ•°ï¼ˆå¦‚ exp, sqrt, floor ç­‰ï¼‰ã€‚

3. [tvm.tir.exp(x)](https://tvm.apache.org/docs/reference/api/python/te.html#tvm.te.exp)

    **ä½œç”¨**
    è®¡ç®— e^xï¼Œå¯¹åº”äºæŒ‡æ•°å‡½æ•°ã€‚å±äº TIR å†…ç½®ç®—å­ã€‚

    æ³¨æ„ç‚¹

    åªèƒ½ç”¨äºç¬¦å·è®¡ç®—ä¸­ï¼›

    ä¸èƒ½ç›´æ¥å¯¹ numpy æ•°ç»„æˆ– Python float è°ƒç”¨ï¼›

    å¦‚æœæƒ³è¦ç¼–è¯‘ååœ¨ runtime å±‚ä½¿ç”¨çœŸå®æ•°å€¼ï¼Œéœ€é€šè¿‡ tvm.build ç”Ÿæˆæ‰§è¡Œæ¨¡å—ã€‚

4. [te.create_prim_func(tensors: List[tvm.te.Tensor])](https://tvm.apache.org/docs/reference/api/python/te.html#tvm.te.create_prim_func)

    **ä½œç”¨**
    å°† TE å±‚çš„è®¡ç®—å®šä¹‰ï¼ˆplaceholder + computeï¼‰è½¬æ¢æˆä¸€ä¸ªå¯è°ƒåº¦çš„ PrimFuncã€‚
    PrimFunc æ˜¯ TVM çš„ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰å±‚å½¢å¼ï¼Œä¾›è°ƒåº¦å™¨å’Œç¼–è¯‘å™¨ä½¿ç”¨ã€‚
    ```
    func = te.create_prim_func([A, C])
    ```

    è¿”å›å€¼

    - ä¸€ä¸ª tvm.ir.PrimFunc å¯¹è±¡ï¼Œå¯ä»¥è¢«è°ƒåº¦æˆ–ç›´æ¥ç¼–è¯‘ã€‚

    æ³¨æ„ç‚¹

    - è¾“å…¥åˆ—è¡¨é¡ºåºå¿…é¡»æ­£ç¡®ï¼ˆå…ˆè¾“å…¥ï¼Œå†è¾“å‡ºï¼‰ï¼›

    - PrimFunc æ˜¯ä¸å¯å˜çš„ï¼ˆimmutableï¼‰ï¼›

    - è¿™æ˜¯è‡ªåŠ¨è°ƒä¼˜æˆ– build ä¹‹å‰çš„å¿…è¦è½¬æ¢æ­¥éª¤ã€‚


### 2.è‡ªåŠ¨è°ƒä¼˜ä¸ç¼–è¯‘ (tvm.meta_schedule, tvm.build)
5. [tvm.meta_schedule.tune_tir(...)](https://tvm.apache.org/docs/reference/api/python/meta_schedule.html#tvm.meta_schedule.tune_tir)

    **ä½œç”¨**

    å¯¹ä¸€ä¸ª PrimFunc è¿›è¡Œ è‡ªåŠ¨è°ƒä¼˜ï¼ˆAuto-Tuningï¼‰ï¼Œæœç´¢æœ€ä¼˜è°ƒåº¦ç­–ç•¥ã€‚
    æ˜¯ TVM MetaSchedule çš„æ ¸å¿ƒ APIã€‚

    ```
    result = ms.tune_tir(
        mod=func,
        target="llvm",
        work_dir="./tune_sigmoid",
        max_trials_global=100,
        num_trials_per_iter=10,
    )
    ```

    **å‚æ•°è¯´æ˜**

    | å‚æ•°                    | å«ä¹‰                                  |
    | --------------------- | ----------------------------------- |
    | `mod`                 | è¦è°ƒä¼˜çš„å‡½æ•°ï¼Œå¯ä»¥æ˜¯ `PrimFunc` æˆ– `IRModule`  |
    | `target`              | è°ƒä¼˜ç›®æ ‡ï¼ˆå¦‚ "llvm", "cuda", "rocm", etcï¼‰ |
    | `work_dir`            | å­˜æ”¾è°ƒä¼˜è®°å½•çš„è·¯å¾„                           |
    | `max_trials_global`   | å…¨å±€æœ€å¤§è°ƒä¼˜æ¬¡æ•°                            |
    | `num_trials_per_iter` | æ¯è½®è¿­ä»£çš„é‡‡æ ·æ¬¡æ•°                           |


    è¿”å›å€¼

    - å¯èƒ½æ˜¯ï¼š

        - tvm.meta_schedule.database.JSONDatabaseï¼ˆä¿å­˜è°ƒä¼˜ç»“æœï¼‰

        - tvm.IRModuleï¼ˆéƒ¨åˆ†æ—§ç‰ˆå…¼å®¹è¿”å› IRModuleï¼‰

    æ³¨æ„ç‚¹

    - è°ƒä¼˜ä¼šè¿è¡Œå¤šæ¬¡å€™é€‰è°ƒåº¦åœ¨ç›®æ ‡è®¾å¤‡ä¸Šå®é™…æ‰§è¡Œï¼›

    - è°ƒä¼˜æ—¶é—´å¯èƒ½å¾ˆé•¿ï¼›

    - å»ºè®®æœ¬åœ° CPU ä½¿ç”¨ llvmï¼ŒGPU ä½¿ç”¨ cudaï¼›

    - æ•°æ®åº“é»˜è®¤åœ¨ work_dir ä¸‹å­˜å‚¨ JSON æ–‡ä»¶ï¼Œå¯å¤ç”¨ã€‚

6. [tvm.meta_schedule.Database(*args: Any, **kwargs: Any)](https://tvm.apache.org/docs/reference/api/python/meta_schedule.html#tvm.meta_schedule.Database)

    **ä½œç”¨**

    å­˜å‚¨å’Œç®¡ç†è‡ªåŠ¨è°ƒä¼˜çš„è®°å½•ç»“æœï¼ˆæ¯ä¸ª workload çš„ scheduleã€æ€§èƒ½ã€å‚æ•°ç­‰ï¼‰ã€‚

    **å¸¸ç”¨æ–¹æ³•ï¼š**
    | æ–¹æ³•                              | ä½œç”¨                         |
    | ------------------------------- | -------------------------- |
    | `.get_all_tuning_records()`     | è·å–æ‰€æœ‰è°ƒä¼˜è®°å½•                   |
    | `.get_top_k(workload, top_k=1)` | è·å–æŸä¸ª workload çš„æ€§èƒ½æœ€ä½³çš„ k æ¡è®°å½• |
    | `.commit_tuning_record(record)` | æ‰‹åŠ¨å†™å…¥ä¸€æ¡è°ƒä¼˜è®°å½•                 |

    æ³¨æ„ç‚¹ï¼š

    - æ¯æ¡è°ƒä¼˜è®°å½•å¯¹åº”ä¸€ä¸ª workloadï¼›

    - å¯ä»¥æŒä¹…åŒ–ä¿å­˜è°ƒä¼˜æ•°æ®ï¼Œä¸‹æ¬¡åŠ è½½ä½¿ç”¨ï¼›

    - ç”¨äºç”Ÿäº§ç¯å¢ƒæ—¶ï¼Œå»ºè®®ä¿å­˜è°ƒä¼˜ç»“æœä»¥å…é‡å¤è®­ç»ƒã€‚

7. [tvm.build(input, target="llvm", name=None)]()

    **ä½œç”¨**

    å°† è°ƒåº¦å¥½çš„è®¡ç®—å›¾ï¼ˆIRModule / Scheduleï¼‰ ç¼–è¯‘æˆå¯æ‰§è¡Œæ¨¡å—ã€‚
    ```
    mod = tvm.build(sch.mod, target="llvm")
    ```

    **å‚æ•°è¯´æ˜**

    | å‚æ•°       | å«ä¹‰                                  |
    | -------- | ----------------------------------- |
    | `input`  | å¯ä»¥æ˜¯ IRModuleã€PrimFunc æˆ– Schedule å¯¹è±¡ |
    | `target` | ç¼–è¯‘ç›®æ ‡ï¼ˆ"llvm"ã€"cuda"ã€"rocm" ç­‰ï¼‰        |
    | `name`   | å¯é€‰æ¨¡å—åç§°                              |

    è¿”å›å€¼

    - tvm.runtime.Moduleï¼Œå¯åœ¨ Python ä¸­ç›´æ¥è°ƒç”¨ã€‚

    æ³¨æ„ç‚¹

    - target å¿…é¡»ä¸è¿è¡Œè®¾å¤‡ä¸€è‡´ï¼›

    - ä¸åŒ target æœ‰ä¸åŒçš„åç«¯ç¼–è¯‘å™¨ï¼›

    - æ„å»ºå®Œæˆçš„æ¨¡å—å¯ä»¥åºåˆ—åŒ–ä¿å­˜ï¼Œå¤ç”¨ã€‚

8. [best_tuning_record.as_measure_candidate()]()

    **ä½œç”¨**

    ä»è°ƒä¼˜è®°å½•ä¸­æå–ä¸€ä¸ªå¯æ‰§è¡Œå€™é€‰ï¼ˆåŒ…å« schedule ä¸å‚æ•°ï¼‰ã€‚

    æ³¨æ„ç‚¹

    - è¿”å›ä¸€ä¸ª MeasureCandidate å¯¹è±¡ï¼›

    - é€šè¿‡ candidate.sch è·å–è°ƒåº¦ï¼›

    - é€šå¸¸åœ¨ tune_tir ä¹‹åï¼Œä»æ•°æ®åº“æå–æœ€ä¼˜è°ƒåº¦æ—¶ä½¿ç”¨ã€‚


### 3.æ‰§è¡Œä¸æµ‹è¯• (tvm.runtime)
9. [tvm.device(target, dev_id=0)]()

    **ä½œç”¨**

    è·å–ä¸€ä¸ªè®¾å¤‡å¯¹è±¡ï¼Œç”¨äºè¿è¡Œç¼–è¯‘æ¨¡å—ã€‚
    ```
    dev = tvm.device("llvm", 0)
    ```

    **å‚æ•°è¯´æ˜**

    | å‚æ•°       | å«ä¹‰                                   |
    | -------- | ------------------------------------ |
    | `target` | è®¾å¤‡ç±»å‹ï¼Œå¦‚ `"llvm"`, `"cuda"`, `"metal"` |
    | `dev_id` | è®¾å¤‡ç¼–å·ï¼ˆé€šå¸¸ä¸º 0ï¼‰                          |


    è¿”å›å€¼

    - ä¸€ä¸ª tvm.runtime.Device å¯¹è±¡ã€‚

    æ³¨æ„ç‚¹

    - target å¿…é¡»ä¸ build æ—¶ä¸€è‡´ï¼›

    - GPU ä¸Šéœ€ç¡®ä¿é©±åŠ¨å’Œ CUDA ç¯å¢ƒæ­£å¸¸ã€‚

10. tvm.runtime.tensor(data, device)

     **ä½œç”¨**

    å°† NumPy æ•°ç»„æˆ– Python æ•°æ®åˆ›å»ºä¸º TVM è¿è¡Œæ—¶å¼ é‡ã€‚
    ```
    a_tvm = tvm.runtime.tensor(a_np, device=dev)
    ```

    æ³¨æ„ç‚¹

    - dtype ä¼šæ ¹æ® numpy æ•°ç»„è‡ªåŠ¨æ¨æ–­ï¼›

    - åˆ†é…åœ¨æŒ‡å®š device ä¸Šï¼›

    - æ˜¯ runtime çº§åˆ«çš„å®é™…æ•°æ®è½½ä½“ï¼ˆå¯æ‰§è¡Œï¼‰ã€‚

11. mod(a_tvm, c_tvm)

    **ä½œç”¨**
    è¿è¡Œç¼–è¯‘å¥½çš„ TVM æ¨¡å—ã€‚

    è¯´æ˜

    - TVM ä¼šè‡ªåŠ¨åŒ¹é…è¾“å…¥è¾“å‡ºå‚æ•°ï¼›

    - æ‰§è¡Œæ—¶è‡ªåŠ¨è°ƒç”¨å¯¹åº” target çš„åç«¯ï¼›

    - å®Œæˆåï¼Œè¾“å‡ºå¼ é‡ä¸­å­˜å‚¨è®¡ç®—ç»“æœã€‚

12. dev.sync()

    **ä½œç”¨**

    ç­‰å¾…è®¾å¤‡ä¸Šæ‰€æœ‰è®¡ç®—å®Œæˆã€‚å¸¸ç”¨äºæ€§èƒ½æµ‹è¯•å‰åä¿è¯åŒæ­¥ã€‚

    æ³¨æ„ç‚¹

    - å¯¹ CPU æ¥è¯´å‡ ä¹æ— å½±å“ï¼›

    - å¯¹ GPUã€å¼‚æ­¥è®¾å¤‡æ¥è¯´å¾ˆé‡è¦ï¼Œå¦åˆ™è®¡æ—¶ä¸å‡†ç¡®ã€‚
    
---
2004

> **ä½œè€…ï¼š** ChatGpt\
>**æäº¤è€…ï¼š** ç‹èƒ¤å‰\
> **æ›´æ–°æ—¥æœŸ:** 2025/11/3


